{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\nfrom sklearn.metrics import accuracy_score,r2_score,mean_squared_error,mean_absolute_error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"file_path\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering\n1.Feature Transformation\n\n2.Feature construction\n\n3.Feature selection\n\n4.Feature Extraction","metadata":{}},{"cell_type":"code","source":"# Feature Transformation\n# a.Handling missing Values\nfrom sklearn.impute import KNNImputer,MissingIndicator,SimpleImputer,IterativeImputer # This are used to impute numerical dtypes for object dtype you need to ysed mode,median\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# b.Handling categorical features\nfrom sklearn.preprocessing import LabelEncoder,OrdinalEncoder,OneHotEncoder  # Label is used for output column,Ordinal is used for grade and onehot is used for Gender","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# or another method is dummies\ndf = pd.get_dummies(data=df,df[\"\"],drop_first=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# c.outlier detection\nsns.boxplot()   # To check the outliers\nsns.distplot()  # To check the flow of skewed of the data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If the data is right skewed then we have to use IQR\n\nIf the data is nrml distributed then we have to use z-score","metadata":{}},{"cell_type":"code","source":"percentile75 = df[\"column_name\"].quantile(0.75)\npercentile25 = df[\"column_name\"].quantile(0.25)\n\niqr = percentile75 - percentile25\n\niqr","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To know the upper and lower limit \nupper = percentile75 + 1.5 * iqr\nlower = percentile25 - 1.5 * iqr\nprint(upper)\nprint(lower)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To perform Z-score method then we use\n\nh = df[\"column_name\"].mean()\ni = df[\"column_name\"].std()\n\nupper = h + 3 * i\n\nlower = h - 3 * i\n\nprint(upper)\nprint(lower)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To Trime the outliers we use\ndf1 = df[(df[\"column_name\"]<upper) & (df[\"column_name\"]>lower)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To perfrom capping of the outlier then we use\ndf[\"new_name\"] = np.where(df[\"column_name\"]>upper,upper,np.where(df[\"column_name\"]<lower,lower,df[\"column_name\"]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# d.Feature Scaling\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler  #MinMaxScaler bring the value in 0 and 1 and standardscaler bring the values between mu=1 std =0\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## # Feature Transformation is further classified into Mathematical transformation ","metadata":{}},{"cell_type":"code","source":"# a. log transformation (if the data is right skwed then we used this transformation)\nfrom sklearn.preprocessing import FunctionTransformer\nft = FunctionTransformer(func = np.log1p)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# we also use power transformer to make the data more undrstandable by ML model (yeo john method as it favor both +ve and -ve values)","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Construction","metadata":{}},{"cell_type":"code","source":"# In this we are dealing with Feature spliting eg : room1 + room2 = calculate as sqft\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature selection","metadata":{}},{"cell_type":"code","source":"# This is related to COD curve of dimensionality\nfrom sklearn.decomposition import PCA ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Extraction","metadata":{}},{"cell_type":"code","source":"# This is also related for COD \nfrom sklearn.decomposition import PCA # This help in reducing the dimensionality of the data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now lets perfrom model building using Machine Learning ALGO","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\ny_pred = lr.predict(x_test)\nprint(\"The Accuracy_score is:\")\naccuracy_score(y_pred,y_test)\nprint(\"the cross val score is:\")\ncross_val_score(lr,x_train,y_train,cv=5).mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## To find the best hyper parameter tuning we perform GridSearchCV","metadata":{}},{"cell_type":"code","source":"n_estimators = [20,60,100,120]\nmax_sample = [0.5,0.75,1.0]\nmax_depth = [2,8,None]\nmax_features = [0.2,0.6,1.0]\n\npara_grid = {\"n_estimators\":n_estimators,\n             \"max_features\":max_features,\n             \"max_depth\":max_depth,\n             \"max_features\":max_features\n             }\n\nlr = LogisticRegression()\nlr_grid = GridSearchCV(estimator=lr,para_grid=para_grid,cv=5,verbose=1,n_jobs=-1)\nlr_grid.fit(x_train,y_train)\nlr_grid.best_params_\nlr_grid.best_score_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## To Deal With Overfitting Problem we are here using Ridge which is also known as Regularization in Machine Learning","metadata":{}},{"cell_type":"code","source":"ridge_model = Ridge(alpha=1.0)  # In this we are setting the alpha value which is equal to 1.0\nridge_model.fit(x_train,y_train)\ny_pred = ridge_model.predict(x_test)\nprint(\"The Accuracy is:\",y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## In this we are using RandomForestClassifier algorithm","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(x_train_pca,y_train)\ny_pred2 = rf.predict(x_test_pca)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The Accuracy_Score of RandomForestClassifier is:\",accuracy_score(y_pred2,y_test))\nprint(\"The cross_val_score of RandomForestClassifer is:\",cross_val_score(rf,x_train_pca,y_train,cv=5).mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's Began with the most popular BoostingClassifier(ensemble)","metadata":{}},{"cell_type":"code","source":"ada = AdaBoostClassifier(n_estimators=1,learning_rate=1.0)\nada.fit(x_train_pca,y_train)\ny_pred_ada = ada.predict(x_test_pca)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The AccuracyScore of AdaBoostClassifier is:\",accuracy_score(y_pred_ada,y_test))\nprint(\"The Cross_Val_Score of AdaBoostClassifier is:\",cross_val_score(ada,x_train_pca,y_train,cv=5).mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now lets perform BaggingClassifier(ensemble)","metadata":{}},{"cell_type":"code","source":"bag = BaggingClassifier(n_estimators=50,estimator=SVC())\nbag.fit(x_train_pca,y_train)\ny_pred_bag = bag.predict(x_test_pca)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The AccuracyScore of BaggingClassifier is:\",accuracy_score(y_pred_bag,y_test))\nprint(\"The Cross_Val_Score of BaggingClassifier is:\",cross_val_score(bag,x_train_pca,y_train,cv=5).mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now lets perform VotingClassifier(ensemble) In this we will take take all algorithm prediction and give the output","metadata":{}},{"cell_type":"code","source":"cf1 = LogisticRegression()\ncf2 = RandomForestClassifier()\ncf3 = SVC(probability=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimators = [(\"lr\",cf1),(\"rf\",cf2),(\"sv\",cf3)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vc = VotingClassifier(estimators=estimators,voting=\"soft\")\nvc.fit(x_train_pca,y_train)\ny_pred_vc = vc.predict(x_test_pca)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The Accuracy_score of VotingClassifier is:\",accuracy_score(y_pred_vc,y_test))\nprint(\"The cross_val_score of VotingClassifier is:\",cross_val_score(vc,x_train_pca,y_train,cv=5).mean())","metadata":{},"execution_count":null,"outputs":[]}]}